{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bb33c6c-d58c-4a43-a5ae-e94575ec99fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "def connect_mongo():\n",
    "    client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = client[\"Ramsaikumardb\"]\n",
    "    collection = db[\"pdf_path\"]\n",
    "    return collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8c08431-48c8-465b-a130-423c42348d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "def download_pdf(pdf_url, output_folder):\n",
    "    try:\n",
    "        response = requests.get(pdf_url)\n",
    "        filename = os.path.join(output_folder, pdf_url.split('/')[-1].split('?')[0])\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return filename\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {pdf_url}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b16c6c72-fd83-4cb4-9082-b8ac5c3e01e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def parse_pdf(file_path):\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            text += reader.pages[page_num].extract_text()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {file_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6600647-dc79-4d81-bc0a-62c913bdff88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ramsaikumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ramsaikumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def summarize_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    if len(sentences) > 5:\n",
    "        return \" \".join(sentences[:5])  # Taking first 5 sentences as summary for simplicity\n",
    "    else:\n",
    "        return text  # Return whole text if it's short\n",
    "\n",
    "def extract_keywords(text, num_keywords=5):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text.lower())\n",
    "    filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    freq_dist = nltk.FreqDist(filtered_words)\n",
    "    return [word for word, freq in freq_dist.most_common(num_keywords)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ef69d4b-93b8-4037-8ac3-54510e1a8a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_initial_data_in_mongo(file_path, collection):\n",
    "    file_metadata = {\n",
    "        \"document_name\": os.path.basename(file_path),\n",
    "        \"path\": file_path,\n",
    "        \"size\": os.path.getsize(file_path),\n",
    "        \"ingestion_date\": datetime.utcnow(),\n",
    "        \"summary\": None,\n",
    "        \"keywords\": None\n",
    "    }\n",
    "    collection.insert_one(file_metadata)\n",
    "    return file_metadata\n",
    "\n",
    "def update_mongo_with_summary(file_metadata, summary, keywords, collection):\n",
    "    query = {\"path\": file_metadata[\"path\"]}\n",
    "    update = {\"$set\": {\"summary\": summary, \"keywords\": keywords}}\n",
    "    collection.update_one(query, update)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7fb98fa-53f0-4ccc-be18-8a0b96b322c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "def process_pdfs_from_dataset(dataset_file, output_folder):\n",
    "    # Load dataset\n",
    "    with open(dataset_file, 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "    \n",
    "    # Connect to MongoDB\n",
    "    collection = connect_mongo()\n",
    "    \n",
    "    for pdf_name, pdf_url in dataset.items():\n",
    "        # Download PDF\n",
    "        file_path = download_pdf(pdf_url, output_folder)\n",
    "        if not file_path:\n",
    "            continue  # Skip if the file couldn't be downloaded\n",
    "        \n",
    "        # Parse PDF\n",
    "        text = parse_pdf(file_path)\n",
    "        if not text:\n",
    "            continue  # Skip if parsing failed\n",
    "        \n",
    "        # Store metadata in MongoDB\n",
    "        file_metadata = store_initial_data_in_mongo(file_path, collection)\n",
    "        \n",
    "        # Generate summary and keywords\n",
    "        summary = summarize_text(text)\n",
    "        keywords = extract_keywords(text)\n",
    "        \n",
    "        # Update MongoDB with summary and keywords\n",
    "        update_mongo_with_summary(file_metadata, summary, keywords, collection)\n",
    "\n",
    "    print(\"All PDFs processed and stored.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a32b70db-cf19-45bc-8354-f8162dada64e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (1128251287.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[21], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    dataset_file = \"C:\\Users\\Ramsaikumar\\OneDrive\\Desktop\\wasserstof\"\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "# Setting the the path to the dataset file and output folder\n",
    "dataset_file = \"C:\\Users\\Ramsaikumar\\OneDrive\\Desktop\\wasserstof\"\n",
    "output_folder = \"/path/to/save/downloaded/pdfs\"  # Replace with an actual folder path\n",
    "\n",
    "# Run the pipeline\n",
    "process_pdfs_from_dataset(dataset_file, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f9176eb-0f22-4a1c-b423-f8d790b8c344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ramsaikumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ramsaikumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pdf1 from https://digiscr.sci.gov.in/pdf_viewer?dir=YWRtaW4vanVkZ2VtZW50X2ZpbGUvanVkZ2VtZW50X3BkZi8xOTUwL3ZvbHVtZSAxL1BhcnQgSS9Db21taXNzaW9uZXIgb2YgSW5jb21lIFRheCwgV2VzdCBCZW5nYWxfQ2FsY3V0dGEgQWdlbmN5IEx0ZC5fMTY5NzYwNjMxMC5wZGY=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ramsaikumar\\AppData\\Local\\Temp\\ipykernel_7536\\828506726.py:70: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"ingestion_date\": datetime.utcnow(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pdf2 from https://digiscr.sci.gov.in/pdf_viewer?dir=YWRtaW4vanVkZ2VtZW50X2ZpbGUvanVkZ2VtZW50X3BkZi8xOTUyL3ZvbHVtZSAxL1BhcnQgSS90aGUgc3RhdGUgb2YgYmloYXJfbWFoYXJhamFkaGlyYWphIHNpciBrYW1lc2h3YXIgc2luZ2ggb2YgZGFyYmhhbmdhIGFuZCBvdGhlcnNfMTY5ODMxODQ0OC5wZGY=\n",
      "Processing pdf3 from https://cdnbbsr.s3waas.gov.in/s380537a945c7aaa788ccfcdf1b99b5d8f/uploads/2024/07/20240716890312078.pdf\n",
      "Processing pdf4 from https://www.mha.gov.in/sites/default/files/250883_english_01042024.pdf\n",
      "Processing pdf5 from https://rbidocs.rbi.org.in/rdocs/PressRelease/PDFs/PR60974A2ED1DFDB84EC0B3AABFB8419E1431.PDF\n",
      "Processing pdf6 from https://digiscr.sci.gov.in/pdf_viewer?dir=YWRtaW4vanVkZ2VtZW50X2ZpbGUvanVkZ2VtZW50X3BkZi8xOTYwL3ZvbHVtZSAxL1BhcnQgSS90aGUgdGF0YSBvaWwgbWlsbHMgY28uIGx0ZC5faXRzIHdvcmttZW4gYW5kIG90aGVyc18xNjk5MzMzODYyLnBkZg==\n",
      "Processing pdf7 from https://digiscr.sci.gov.in/pdf_viewer?dir=YWRtaW4vanVkZ2VtZW50X2ZpbGUvanVkZ2VtZW50X3BkZi8xOTYwL3ZvbHVtZSAxL1BhcnQgSS9ncmVhdCBpbmRpYW4gbW90b3Igd29ya3MgbHRkLiwgYW5kIGFub3RoZXJfdGhlaXIgZW1wbG95ZWVzIGFuZCBvdGhlcnNfMTY5OTMzNjM1NS5wZGY=\n",
      "Processing pdf8 from https://digiscr.sci.gov.in/pdf_viewer?dir=YWRtaW4vanVkZ2VtZW50X2ZpbGUvanVkZ2VtZW50X3BkZi8xOTYwL3ZvbHVtZSAxL1BhcnQgSS9tZXNzcnMuIGlzcGFoYW5pIGx0ZC4gY2FsY3V0dGFfaXNwYWhhbmkgZW1wbG95ZWVzICB1bmlvbl8xNjk5MzM4NTQ5LnBkZg==\n",
      "Processing pdf9 from https://digiscr.sci.gov.in/pdf_viewer?dir=YWRtaW4vanVkZ2VtZW50X2ZpbGUvanVkZ2VtZW50X3BkZi8xOTYwL3ZvbHVtZSAxL1BhcnQgSS9waHVsYmFyaSB0ZWEgZXN0YXRlX2l0cyB3b3JrbWVuXzE2OTkzMzkyMjYucGRm\n",
      "Processing pdf10 from https://digiscr.sci.gov.in/pdf_viewer?dir=YWRtaW4vanVkZ2VtZW50X2ZpbGUvanVkZ2VtZW50X3BkZi8xOTYwL3ZvbHVtZSAxL1BhcnQgSS90aGUgbG9yZCBrcmlzaG5hIHN1Z2FyIG1pbGxzIGx0ZC4sIGFuZCBhbm90aGVyX3RoZSB1bmlvbiBvZiBpbmRpYSBhbmQgYW5vdGhlcl8xNjk5MzQxMDE0LnBkZg==\n",
      "Processing pdf11 from https://digiscr.sci.gov.in/pdf_viewer?dir=YWRtaW4vanVkZ2VtZW50X2ZpbGUvanVkZ2VtZW50X3BkZi8xOTYwL3ZvbHVtZSAxL1BhcnQgSS8xNjk5NTIxMzUwLnBkZg==\n",
      "Processing pdf12 from https://digiscr.sci.gov.in/pdf_viewer?dir=YWRtaW4vanVkZ2VtZW50X2ZpbGUvanVkZ2VtZW50X3BkZi8xOTYwL3ZvbHVtZSAxL1BhcnQgSS90aGUgZ3JhaGFtIHRyYWRpbmcgY28uIChpbmRpYSkgbHRkLl9pdHMgd29ya21lbl8xNjk5NTIzNTc3LnBkZg==\n",
      "Processing pdf13 from https://digiscr.sci.gov.in/pdf_viewer?dir=YWRtaW4vanVkZ2VtZW50X2ZpbGUvanVkZ2VtZW50X3BkZi8xOTYwL3ZvbHVtZSAxL1BhcnQgSS90aGUgY29tbWlzc2lvbmVyIG9mIGluY29tZS10YXgsIGJvbWJheV9yYW5jaGhvZGRhcyBrYXJzb25kYXMsIGJvbWJheV8xNjk5NTI2MjI3LnBkZg==\n",
      "Processing pdf14 from https://digiscr.sci.gov.in/pdf_viewer?dir=YWRtaW4vanVkZ2VtZW50X2ZpbGUvanVkZ2VtZW50X3BkZi8xOTYwL3ZvbHVtZSAxL1BhcnQgSS8xNjk5NTI2ODA0LnBkZg==\n",
      "Processing pdf15 from https://digiscr.sci.gov.in/pdf_viewer?dir=YWRtaW4vanVkZ2VtZW50X2ZpbGUvanVkZ2VtZW50X3BkZi8xOTYwL3ZvbHVtZSAxL1BhcnQgSS9zaHJpIGIuIHAuIGhpcmEsIHdvcmtzIG1hbmFnZXIsIGNlbnRyYWwgcmFpbHdheSwgcGFyZWwsIGJvbWJheSBldGMuX3NocmkgYy4gbS4gcHJhZGhhbiBldGMuXzE2OTk1MjcyMTcucGRm\n",
      "Processing pdf16 from https://www.sebi.gov.in/sebi_data/attachdocs/1292585113260.pdf\n",
      "Processing pdf17 from https://ijtr.nic.in/Circular%20Orders%20(Supplement).pdf\n",
      "Error downloading https://ijtr.nic.in/Circular%20Orders%20(Supplement).pdf: HTTPSConnectionPool(host='ijtr.nic.in', port=443): Max retries exceeded with url: /Circular%20Orders%20(Supplement).pdf (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "Processing pdf18 from https://enforcementdirectorate.gov.in/sites/default/files/Act%26rules/The%20Prevention%20of%20Money-laundering%20%28Maintenance%20of%20Records%29%20Rules%2C%202005.pdf\n",
      "All PDFs processed and stored.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pymongo import MongoClient\n",
    "import requests\n",
    "from PyPDF2 import PdfReader\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from datetime import datetime\n",
    "\n",
    "# Download NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# MongoDB connection function\n",
    "def connect_mongo():\n",
    "    client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = client[\"Ramsaikumardb\"]\n",
    "    collection = db[\"pdf_path\"]\n",
    "    return collection\n",
    "\n",
    "# Function to download a PDF from a URL\n",
    "def download_pdf(pdf_url, output_folder):\n",
    "    try:\n",
    "        response = requests.get(pdf_url)\n",
    "        # Extract filename from URL\n",
    "        filename = os.path.join(output_folder, pdf_url.split('/')[-1].split('?')[0])\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return filename\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {pdf_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to parse text from a PDF file\n",
    "def parse_pdf(file_path):\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            text += reader.pages[page_num].extract_text()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to summarize text\n",
    "def summarize_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    if len(sentences) > 5:\n",
    "        return \" \".join(sentences[:5])  # Taking first 5 sentences as summary\n",
    "    else:\n",
    "        return text  # Return whole text if it's short\n",
    "\n",
    "# Function to extract keywords from text\n",
    "def extract_keywords(text, num_keywords=5):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text.lower())\n",
    "    filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "    freq_dist = nltk.FreqDist(filtered_words)\n",
    "    return [word for word, freq in freq_dist.most_common(num_keywords)]\n",
    "\n",
    "# Function to store initial data in MongoDB\n",
    "def store_initial_data_in_mongo(file_path, collection):\n",
    "    file_metadata = {\n",
    "        \"document_name\": os.path.basename(file_path),\n",
    "        \"path\": file_path,\n",
    "        \"size\": os.path.getsize(file_path),\n",
    "        \"ingestion_date\": datetime.utcnow(),\n",
    "        \"summary\": None,\n",
    "        \"keywords\": None\n",
    "    }\n",
    "    collection.insert_one(file_metadata)\n",
    "    return file_metadata\n",
    "\n",
    "# Function to update MongoDB with summary and keywords\n",
    "def update_mongo_with_summary(file_metadata, summary, keywords, collection):\n",
    "    query = {\"path\": file_metadata[\"path\"]}\n",
    "    update = {\"$set\": {\"summary\": summary, \"keywords\": keywords}}\n",
    "    collection.update_one(query, update)\n",
    "\n",
    "# Main function to process PDFs from dataset\n",
    "def process_pdfs_from_dataset(dataset_file, output_folder):\n",
    "    # Load dataset\n",
    "    with open(dataset_file, 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "    \n",
    "    # Connect to MongoDB\n",
    "    collection = connect_mongo()\n",
    "    \n",
    "    # Ensure output folder exists\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for pdf_name, pdf_url in dataset.items():\n",
    "        print(f\"Processing {pdf_name} from {pdf_url}\")\n",
    "        \n",
    "        # Download PDF\n",
    "        file_path = download_pdf(pdf_url, output_folder)\n",
    "        if not file_path:\n",
    "            continue  # Skip if the file couldn't be downloaded\n",
    "        \n",
    "        # Parse PDF\n",
    "        text = parse_pdf(file_path)\n",
    "        if not text:\n",
    "            continue  # Skip if parsing failed\n",
    "        \n",
    "        # Store metadata in MongoDB\n",
    "        file_metadata = store_initial_data_in_mongo(file_path, collection)\n",
    "        \n",
    "        # Generate summary and keywords\n",
    "        summary = summarize_text(text)\n",
    "        keywords = extract_keywords(text)\n",
    "        \n",
    "        # Update MongoDB with summary and keywords\n",
    "        update_mongo_with_summary(file_metadata, summary, keywords, collection)\n",
    "    \n",
    "    print(\"All PDFs processed and stored.\")\n",
    "\n",
    "# Set the path to the dataset file and output folder\n",
    "dataset_file = r\"C:\\Users\\Ramsaikumar\\OneDrive\\Desktop\\wasserstof\\Dataset.json\"  # Path to the JSON dataset\n",
    "output_folder = r\"C:\\Users\\Ramsaikumar\\OneDrive\\Desktop\\wasserstof\"  # Path to the output folder\n",
    "\n",
    "# Run the pipeline\n",
    "process_pdfs_from_dataset(dataset_file, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd476d28-655c-41f0-8717-ee17fe95b8be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
